NCL_Validation~., data = entrena[,1:6], method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(model)
summary(model)
model$bestTune
# Summarize the final model
summary(model$finalModel)
predictions <- model %>% predict(data)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, data$NCL_Validation),
Rsquare = caret::R2(predictions, data$NCL_Validation)
)
predictions
install.packages("broom")
library(tidyverse)
library(broom)
theme_set(theme_classic())
#construir un modelo de regresión logistica
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#construir un modelo de regresión logistica
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
#construir un modelo de regresión logistica
# Load the data
install.packages("mlbench")
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
library(GGally)
##Using ggplot
library(ggplot2)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=diabetes))
library(GGally)
##Using ggplot
library(ggplot2)
install.packages("GGally")
##Using ggplot
install.packages("ggplot2")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=diabetes))
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpair(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
#se instalan las librerias y se cargan los paquetes
install.packages("Rtools")
##########  REGRESIÓN LOGISTICA ###############################
version
library(tidyverse)
library(broom)
theme_set(theme_classic())
##########  REGRESIÓN LOGISTICA ###############################
version
install.packages("Rtools")
install.packages("tidiverse")
install.packages("broom")
library(tidyverse)
library(broom)
theme_set(theme_classic())
# Load the data
install.packages("mlbench")
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
##########  REGRESIÓN LOGISTICA ###############################
version
install.packages("installr")
library(installr)
updateR()
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
##Using ggplot
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
##########  REGRESIÓN LOGISTICA ###############################
version
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
version
version
install.packages("Rtools")
install.packages("tidiverse")
install.packages("broom")
library(tidyverse)
library(broom)
theme_set(theme_classic())
install.packages("mlbench")
library(mlbench)
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
##Using ggplot
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8,
mapping=aes(color=PimaIndiansDiabetes2$diabetes), alpha=0.5)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8,
mapping=aes(color=PimaIndiansDiabetes2$diabetes,alpha=0.5) )
#se agregan líneas de regresión
ggpairs(data=PimaIndiansDiabetes2, columns=1:8,
mapping=aes(color=PimaIndiansDiabetes2$diabetes,alpha=0.5),
lower = list(continuous = "smooth") )
ggcorr(PimaIndiansDiabetes2[,-1], palette = "RdBu", label = TRUE)
# Fit the logistic regression model
model <- glm(diabetes ~., data = PimaIndiansDiabetes2,
family = binomial)
print(model)
# Predict the probability (p) of diabete positivity
probabilities <- predict(model, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)
plot(pobabilities)
probabilities
head(probabilities)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)
mydata <- PimaIndiansDiabetes2 %>%
dplyr::select_if(is.numeric)
predictors <- colnames(mydata)
mydata <- mydata %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key = "predictors", value = "predictor.value", -logit)
predictors <- colnames(mydata)
mydata <- mydata %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key = "predictors", value = "predictor.value", -logit)
# Bind the logit and tidying the data for plot
install.packages("tidyr")
install.packages("tidyr")
library(tidyr)
mydata <- mydata %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key = "predictors", value = "predictor.value", -logit)
install.packages("tidyverse")
library(tidyverse)
mydata <- mydata %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key = "predictors", value = "predictor.value", -logit)
#create scatterplots
ggplot(mydata, aes(logit, predictor.value))+
geom_point(size = 0.5, alpha = 0.5) +
geom_smooth(method = "loess") +
theme_bw() +
facet_wrap(~predictors, scales = "free_y")
#valores de influencia (outliers)
plot(model, which = 4, id.n = 3)
model.data <- augment(model) %>%
mutate(index = 1:n())
#The data for the top 3 largest values, according to the Cook’s distance,
#can be displayed as follow:
model.data %>% top_n(3, .cooksd)
#Plot the standardized residuals:
ggplot(model.data, aes(index, .std.resid)) +
geom_point(aes(color = diabetes), alpha = .5) +
theme_bw()
#Filter potential influential data points with abs(.std.res) > 3:
model.data %>%
filter(abs(.std.resid) > 3)
###################################################################
install.packages("caret")
library(caret)
install.packages("lattice")
install.packages("lattice")
library(lattice)
library(caret)
library(tidyverse)
# se dividen los datos en datos de Entrenamiento y prueba
set.seed(123)
training.samples <- PimaIndiansDiabetes2$diabetes %>%
createDataPartition(p = 0.8, list = FALSE)
train.data  <- PimaIndiansDiabetes2[training.samples, ]
test.data <- PimaIndiansDiabetes2[-training.samples, ]
library(MASS)
# Fit the model
model <- glm(diabetes ~., data = train.data, family = binomial) %>%
stepAIC(trace = FALSE)
# Summarize the final selected model
summary(model)
# Model accuracy
mean(predicted.classes==test.data$diabetes)
full.model <- glm(diabetes ~., data = train.data, family = binomial)
coef(full.model)
# Summarize the final selected model
summary(model)
coef(full.model)
step.model <- full.model %>% stepAIC(trace = FALSE)
coef(step.model)
#Compare the full and the stepwise models
# Make predictions
probabilities <- full.model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Prediction accuracy
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
probabilities <- predict(step.model, test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Prediction accuracy
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
error <- mean(observed.classes != predicted.classes)
error
###confusion matrix
# Confusion matrix, number of cases
table(observed.classes, predicted.classes)
# Confusion matrix, proportion of cases
table(observed.classes, predicted.classes) %>%
prop.table() %>% round(digits = 3)
#Precision, Recall and Specificity
confusionMatrix(predicted.classes, observed.classes,
positive = "pos")
#Precision, Recall and Specificity
confusionMatrix(predicted.classes, observed.classes)
probabilities <- full.model %>% predict(test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
# Prediction accuracy
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
probabilities <- predict(step.model, test.data, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
observed.classes <- test.data$diabetes
mean(predicted.classes == observed.classes)
error <- mean(observed.classes != predicted.classes)
error
###confusion matrix
# Confusion matrix, number of cases
table(observed.classes, predicted.classes)
table(observed.classes, predicted.classes) %>%
prop.table() %>% round(digits = 3)
#Precision, Recall and Specificity
confusionMatrix(predicted.classes, observed.classes)
#Precision, Recall and Specificity
confusionMatrix(as.factor(predicted.classes), observed.classes)
### ROC CURVE
install.packages("pROC")
install.packages("pROC")
library(pROC)
# Compute roc
res.roc <- roc(observed.classes, prediction.probabilities)
# Compute roc
res.roc <- roc(observed.classes, predicted.classes)
plot.roc(res.roc, print.auc = TRUE)
library(pROC)
# Compute roc
res.roc <- roc(observed.classes, predicted.classes)
# Compute roc
res.roc <- roc(observed.classes, prediction.probabilities)
plot.roc(res.roc, print.auc = TRUE)
# Compute roc
res.roc <- roc(observed.classes, probabilities)
plot.roc(res.roc, print.auc = TRUE)
res.roc <- roc(observed.classes, probabilities)
plot.roc(res.roc, print.auc = TRUE)
# Extract some interesting results
roc.data <- data_frame(
thresholds = res.roc$thresholds,
sensitivity = res.roc$sensitivities,
specificity = res.roc$specificities
)
# Get the probality threshold for specificity = 0.6
roc.data %>% filter(specificity >= 0.6)
# Get the probality threshold for specificity = 0.6
roc.data %>% filter(specificity >= 0.6)
#The best threshold with the highest sum sensitivity + specificity can be
#printed as follow. There might be more than one threshold.
plot.roc(res.roc, print.auc = TRUE, print.thres = "best")
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting classifier to the Training set
# Create your classifier here
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-3])
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3],
main = 'Classifier (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = predict(classifier, newdata = grid_set)
plot(set[, -3], main = 'Classifier (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
getwd("C:/Users/javij/OneDrive/Documentos/GitHub\Material-CNBV/MOD 8/Datasets")
getwd("C:/Users/javij/OneDrive/Documentos/GitHub\Material-CNBV/MOD 8/Datasets")
# Importing the dataset
dataset = read.csv('Social_Network_Ads.csv')
setwd("C:/Users/javij/OneDrive/Documentos/GitHub\Material-CNBV/MOD 8/Datasets")
setwd("C:\Users\javij\OneDrive\Documentos\GitHub\Material-CNBV\MOD 8\Datasets")
getwd()
setwd("C:/Users/javij/OneDrive/Documentos/GitHub/Material-CNBV/MOD 8/Datasets")
dataset = read.csv('Social_Network_Ads.csv')
dataset = dataset[3:5]
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
# install.packages('caTools')
library(caTools)
# Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting K-NN to the Training set and Predicting the Test set results
library(class)
y_pred = knn(train = training_set[, -3],
test = test_set[, -3],
cl = training_set[, 3],
k = 5,
prob = TRUE)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
# Visualising the Training set results
library(ElemStatLearn)
# Visualising the Training set results
install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3], test = grid_set, cl = training_set[, 3], k = 5)
plot(set[, -3],
main = 'K-NN (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3], test = grid_set, cl = training_set[, 3], k = 5)
plot(set[, -3],
main = 'K-NN (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
error_rate = []
for i in range(1,50):
knn = KNeighborsClassifier(n_neighbors=i)
knn.fit(X_train, y_train)
pred = knn.predict(X_test)
error_rate.append(np.mean(pred != y_test))
plt.figure(figsize=(15,10))
plt.plot(range(1,50),error_rate, marker='o', markersize=9)
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
y_pred = knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = 10,
prob = TRUE)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
library(class)
y_pred = knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = 2,
prob = TRUE)
cm = table(test_set[, 3], y_pred)
print(cm)
library(caret)
dataset = dataset[3:5]
dataset = read.csv('Social_Network_Ads.csv')
head(dataset)
dataset = dataset[3:5]
head(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
y_pred = knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = 2,
prob = TRUE)
confusionMatrix(table(y_pred ,training_set[, 3]))
confusionMatrix(table(y_pred ,training_set[, 3])
library(caret)
confusionMatrix(table(y_pred ,training_set[, 3]))
confusionMatrix(table(y_pred ,test_set[, 3]))
i=1
k.optm=1
for (i in 1:28){
+ knn.mod <- knn(train=train.loan, test=test.loan, cl=train.loan_labels, k=i)
+ k.optm[i] <- 100 * sum(test.loan_labels == knn.mod)/NROW(test.loan_labels)
+ k=i
+ cat(k,'=',k.optm[i],'
')
+ }
+ cat(k,'=',k.optm[i],'
') }
k.optm=1
for (i in 1:28){
knn.mod<- knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = i,
prob = TRUE)
k.optm[i] <- 100 * sum(test_set[, 3] == knn.mod)/NROW(test_set[, 3])
k=i
cat(k,'=',k.optm[i],'')
}
plot(k.optm, type="b", xlab="K- Value",ylab="Accuracy level")
