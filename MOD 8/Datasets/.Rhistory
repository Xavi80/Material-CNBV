print(lmodel)
summary(lmodel)
lmodel$coefficients
beta.z<-as.matrix(lmodel$coefficients)
beta.z
lmodel<-lm(entrena$NCL_Validation~ PC1 + PC2 +PC3 +PC6, data=regresion)
print(lmodel)
summary(lmodel) #SOLO EL PC1, PC2, PC3 y pc6 son significativos
lmodel$coefficients
beta.z<-as.matrix(lmodel$coefficients)
beta.z
beta.z<-as.matrix(lmodel$coefficients)
beta.z
pca$rotation
V<-as.matrix(pca$rotation)
V
beta.x<-V %*% beta.z
lmodel$coefficients
beta.z<-as.matrix(lmodel$coefficients)
beta.z
pca$rotation[,1:3,6]
pca$rotation[,1:3]
lmodel<-lm(entrena$NCL_Validation~ PC1 + PC2 +PC3 +PC6, data=regresion)
print(lmodel)
summary(lmodel) #SOLO EL PC1, PC2, PC3 y pc6 son significativos
lmodel$coefficients
beta.z<-as.matrix(lmodel$coefficients)
pca1<-pca$rotation[,1:3]
pca2<-pca$rotation[,6]
pca1
pca2
pca2
head([, c('pca1', 'pca2')])
pca1<-pca$rotation[,1:3][,6]
pca$rotation
V<-as.matrix(pca$rotation,pca$rotation[,6])
V<-as.matrix(pca$rotation[,1:3],pca$rotation[,6])
V
V<-as.matrix(cbind(pca$rotation[,1:3],pca$rotation[,6]))
V
V<-as.matrix(cbind(pca$rotation[,1:3],pca$rotation[,1]))
V
V<-as.matrix(cbind(pca$rotation[,1:3],pca$rotation[,6:6]))
V
pca$rotation
V<-as.matrix(cbind(pca$rotation[,1:3],pca$rotation[,"PC6"]))
V
pca2
beta.x<-V %*% beta.z
beta.z
beta.z<-as.matrix(lmodel$coefficients)
beta.z
pca$rotation
V<-as.matrix(pca$rotation)
beta.x<-V %*% beta.z
pca<-prcomp(entrena[c(2:7)], scale = TRUE, center=TRUE)
plot(pca)
print(pca)
fviz_pca_var(pca,axes = c(1, 2))
pca$x
regresion<-data.frame(pca$x,entrena$NCL_Validation)
lmodel<-lm(entrena$NCL_Validation~ PC1 + PC2 +PC3 + PC4+PC5+PC6, data=regresion)
print(lmodel)
summary(lmodel) #SOLO EL PC1, PC2, PC3 y pc6 son significativos
lmodel$coefficients
beta.z<-as.matrix(lmodel$coefficients)
beta.z
pca$rotation
V<-as.matrix(pca$rotation)
V
beta.x<-V %*% beta.z
pca<-prcomp(entrena[c(2:7)], scale = TRUE, center=TRUE)
plot(pca)
print(pca)
fviz_pca_var(pca,axes = c(1, 2))
pca$x
regresion<-data.frame(pca$x,entrena$NCL_Validation)
lmodel<-lm(entrena$NCL_Validation~ PC1 + PC2 +PC3 + PC4+PC5+PC6, data=regresion)
print(lmodel)
summary(lmodel) #SOLO EL PC1, PC2, PC3 y pc6 son significativos
lmodel$coefficients
beta.z<-as.matrix(lmodel$coefficients)
beta.z
pca$rotation
V<-as.matrix(pca$rotation)
V
beta.x<-V %*% beta.z
#Matriz de eigenvectores
pca$rotation
#coeficientes de la regresión múltiple usando Componentes principales
print(lmodel$coefficients)
beta0 <- lmodel$coefficients[1]
print(beta0)
betas <- lmodel$coefficients[2:5]
print(betas)
set.seed(123)
model <- train(
NCL_Validation~., data = entrena, method = "pls",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(model)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
model$bestTune
summary(model$finalModel)
# Make predictions
predictions <- model %>% predict(data)
data.frame(
RMSE = caret::RMSE(predictions, test.data$medv),
Rsquare = caret::R2(predictions, test.data$medv)
)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, data$NCL_Validation),
Rsquare = caret::R2(predictions, data$NCL_Validation)
)
predictions
set.seed(123)
model <- train(
NCL_Validation~., data = entrena, method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(model)
# Print the best tuning parameter ncomp that
# minimize the cross-validation error, RMSE
model$bestTune
# Summarize the final model
summary(model$finalModel)
# Make predictions
predictions <- model %>% predict(data)
predictions <- model %>% predict(data)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, data$NCL_Validation),
Rsquare = caret::R2(predictions, data$NCL_Validation)
)
View(entrena)
View(entrena)
set.seed(123)
model <- train(
NCL_Validation~., data = entrena[,1:6], method = "pcr",
scale = TRUE,
trControl = trainControl("cv", number = 10),
tuneLength = 10
)
# Plot model RMSE vs different values of components
plot(model)
summary(model)
model$bestTune
# Summarize the final model
summary(model$finalModel)
predictions <- model %>% predict(data)
# Model performance metrics
data.frame(
RMSE = caret::RMSE(predictions, data$NCL_Validation),
Rsquare = caret::R2(predictions, data$NCL_Validation)
)
predictions
install.packages("broom")
library(tidyverse)
library(broom)
theme_set(theme_classic())
#construir un modelo de regresión logistica
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#construir un modelo de regresión logistica
# Load the data
data("PimaIndiansDiabetes2", package = "mlbench")
#construir un modelo de regresión logistica
# Load the data
install.packages("mlbench")
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
library(GGally)
##Using ggplot
library(ggplot2)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=diabetes))
library(GGally)
##Using ggplot
library(ggplot2)
install.packages("GGally")
##Using ggplot
install.packages("ggplot2")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=diabetes))
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpair(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
#se instalan las librerias y se cargan los paquetes
install.packages("Rtools")
##########  REGRESIÓN LOGISTICA ###############################
version
library(tidyverse)
library(broom)
theme_set(theme_classic())
##########  REGRESIÓN LOGISTICA ###############################
version
install.packages("Rtools")
install.packages("tidiverse")
install.packages("broom")
library(tidyverse)
library(broom)
theme_set(theme_classic())
# Load the data
install.packages("mlbench")
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
##########  REGRESIÓN LOGISTICA ###############################
version
install.packages("installr")
library(installr)
updateR()
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
##Using ggplot
install.packages("ggplot2")
install.packages("GGally")
library(ggplot2)
library(GGally)
##########  REGRESIÓN LOGISTICA ###############################
version
ggpairs(data=PimaIndiansDiabetes2, columns=1:8, mapping=aes(color=PimaIndiansDiabetes2$diabetes))
version
getwd()
setwd("C:/Users/javij/OneDrive/Documentos/GitHub/Material-CNBV/MOD 8/Datasets")
dataset = read.csv('Social_Network_Ads.csv')
head(dataset)
dataset = dataset[3:5]
head(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
# Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
# Fitting K-NN to the Training set and Predicting the Test set results
library(class)
y_pred = knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = 2,
prob = TRUE)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
library(caret)
confusionMatrix(table(y_pred ,test_set[, 3]))
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
library(class)
y_pred = knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = 2,
prob = TRUE)
print(y_pred)
# Making the Confusion Matrix
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
# Making the Confusion Matrix
library(tidyverse)
library(ggplot2)
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
library(caret)
confusionMatrix(table(y_pred ,test_set[, 3]))
# Visualising the Training set results
install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3], test = grid_set, cl = training_set[, 3], k = 5)
plot(set[, -3],
main = 'K-NN (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3], test = grid_set, cl = training_set[, 3], k = 5)
plot(set[, -3],
main = 'K-NN (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
library(tidyverse)
library(broom)
theme_set(theme_classic())
library(mlbench)
data("PimaIndiansDiabetes2", package = "mlbench")
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
head(PimaIndiansDiabetes2)
#elaboramos un análisis d elas variables del dataset
str(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
pairs(PimaIndiansDiabetes2[,-1])
# se le agrega color
pairs(PimaIndiansDiabetes2[,-1],col=PimaIndiansDiabetes2$diabetes)
library(ggplot2)
library(GGally)
ggpairs(data=PimaIndiansDiabetes2, columns=1:8,
mapping=aes(color=PimaIndiansDiabetes2$diabetes,alpha=0.5) )
#se agregan líneas de regresión
ggpairs(data=PimaIndiansDiabetes2, columns=1:8,
mapping=aes(color=PimaIndiansDiabetes2$diabetes,alpha=0.5),
lower = list(continuous = "smooth") )
ggcorr(PimaIndiansDiabetes2[,-1], palette = "RdBu", label = TRUE)
# Fit the logistic regression model
model <- glm(diabetes ~., data = PimaIndiansDiabetes2,
family = binomial)
print(model)
# Predict the probability (p) of diabete positivity
probabilities <- predict(model, type = "response")
head(probabilities)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")
head(predicted.classes)
mydata <- PimaIndiansDiabetes2 %>%
dplyr::select_if(is.numeric)
predictors <- colnames(mydata)
library(tidyverse)
library(tidyr)
mydata <- mydata %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key = "predictors", value = "predictor.value", -logit)
ggplot(mydata, aes(logit, predictor.value))+
geom_point(size = 0.5, alpha = 0.5) +
geom_smooth(method = "loess") +
theme_bw() +
facet_wrap(~predictors, scales = "free_y")
#valores de influencia (outliers)
plot(model, which = 4, id.n = 3)
# Extract model results
model.data <- augment(model) %>%
mutate(index = 1:n())
#The data for the top 3 largest values, according to the Cook's distance,
#can be displayed as follow:
model.data %>% top_n(3, .cooksd)
#Plot the standardized residuals:
ggplot(model.data, aes(index, .std.resid)) +
geom_point(aes(color = diabetes), alpha = .5) +
theme_bw()
#Filter potential influential data points with abs(.std.res) > 3:
model.data %>%
filter(abs(.std.resid) > 3)
print(model.data)
#Filter potential influential data points with abs(.std.res) > 3:
print(model.data %>%
filter(abs(.std.resid) > 3))
#Filter potential influential data points with abs(.std.res) > 3:
influential<-model.data %>%
filter(abs(.std.resid) > 3)
print(influential)
influential
#Filter potential influential data points with abs(.std.res) > 3:
influential<-model.data %>%
filter(abs(.std.resid) > 2)
influential
getwd()
dataset = read.csv('Social_Network_Ads.csv')
head(dataset)
dataset = dataset[3:5]
head(dataset)
str(dataset)
# Encoding the target feature as factor
dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))
str(dataset)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
library(caTools)
set.seed(123)
split = sample.split(dataset$Purchased, SplitRatio = 0.75)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Feature Scaling
training_set[-3] = scale(training_set[-3])
test_set[-3] = scale(test_set[-3])
library(class)
y_pred = knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = 2,
prob = TRUE)
print(y_pred)
library(tidyverse)
library(ggplot2)
cm = table(test_set[, 3], y_pred)
print(cm)
table(test_set[, 3], y_pred) %>%
prop.table() %>% round(digits = 3)
library(caret)
confusionMatrix(table(y_pred ,test_set[, 3]))
# Visualising the Training set results
install.packages("ElemStatLearn")
library(ElemStatLearn)
set = training_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3], test = grid_set, cl = training_set[, 3], k = 5)
plot(set[, -3],
main = 'K-NN (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualising the Test set results
library(ElemStatLearn)
set = test_set
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
y_grid = knn(train = training_set[, -3], test = grid_set, cl = training_set[, 3], k = 5)
plot(set[, -3],
main = 'K-NN (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
i=1
k.optm=1
for (i in 1:28){
knn.mod<- knn(train = training_set[, -3],test = test_set[, -3],
cl = training_set[, 3],
k = i,
prob = TRUE)
k.optm[i] <- 100 * sum(test_set[, 3] == knn.mod)/NROW(test_set[, 3])
k=i
cat(k,'=',k.optm[i],'')
}
#Accuracy plot
plot(k.optm, type="b", xlab="K- Value",ylab="Accuracy level")
version
